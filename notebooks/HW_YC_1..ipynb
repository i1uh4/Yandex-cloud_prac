{
  "metadata": {
    "name": "HW_1",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql import functions as F\n\n\nusers \u003d spark.createDataFrame(\n    [\n        (\"u1\", \"Berlin\"),\n        (\"u2\", \"Berlin\"),\n        (\"u3\", \"Munich\"),\n        (\"u4\", \"Hamburg\"),\n    ],\n    [\"user_id\", \"city\"]\n)\n\norders \u003d spark.createDataFrame(\n    [\n        (\"o1\", \"u1\", \"p1\", 2,  10.0),\n        (\"o2\", \"u1\", \"p2\", 1,  30.0),\n        (\"o3\", \"u2\", \"p1\", 1,  10.0),\n        (\"o4\", \"u2\", \"p3\", 5,   7.0),\n        (\"o5\", \"u3\", \"p2\", 3,  30.0),\n        (\"o6\", \"u3\", \"p3\", 1,   7.0),\n        (\"o7\", \"u4\", \"p1\", 10, 10.0),\n    ],\n    [\"order_id\", \"user_id\", \"product_id\", \"qty\", \"price\"]\n)\n\nproducts \u003d spark.createDataFrame(\n    [\n        (\"p1\", \"Ring VOLA\"),\n        (\"p2\", \"Ring POROG\"),\n        (\"p3\", \"Ring TISHINA\"),\n    ],\n    [\"product_id\", \"product_name\"]\n)\n\nusers.show()\norders.show()\nproducts.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# Посчитаем revenue\norders_with_revenue \u003d orders.withColumn(\n    \"revenue\",\n    F.col(\"qty\") * F.col(\"price\")\n)\n\norders_with_revenue.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# Объединим orders с users и products\norders_enriched \u003d (\n    orders_with_revenue\n    .join(users,    on\u003d\"user_id\",    how\u003d\"left\")\n    .join(products, on\u003d\"product_id\", how\u003d\"left\")\n)\n\norders_enriched.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n### Посчитаем метрики по (city, product_id, product_name):\n### - orders_cnt (кол-во заказов)\n### - qty_sum (сумма qty)\n### - revenue_sum (сумма revenue)\nmart_agg \u003d (\n    orders_enriched\n    .groupBy(\"city\", \"product_id\", \"product_name\")\n    .agg(\n        F.count(\"order_id\").alias(\"orders_cnt\"),\n        F.sum(\"qty\").alias(\"qty_sum\"),\n        F.sum(\"revenue\").alias(\"revenue_sum\")\n    )\n)\n\nmart_agg.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# Для каждого города выбрать Top-2 товара по revenue_sum используя Window\nwindow_spec \u003d Window.partitionBy(\"city\").orderBy(F.desc(\"revenue_sum\"))\n\nmart_with_rank \u003d mart_agg.withColumn(\n    \"rank\",\n    F.row_number().over(window_spec)\n)\n\nmart_city_top_products \u003d mart_with_rank.filter(F.col(\"rank\") \u003c\u003d 2)\n\nmart_city_top_products.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# Теперь сохраним результат в HDFS по пути: /tmp/sandbox_zeppelin/mart_city_top_products/ (parquet, overwrite)\nhdfs_path \u003d \"/tmp/sandbox_zeppelin/mart_city_top_products/\"\n\nmart_city_top_products.write.mode(\"overwrite\").parquet(hdfs_path)\n\nprint(f\"Записано в {hdfs_path}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# Теперь в s3\ns3_path \u003d \"s3a://hw-1-yc/mart_city_top_products/\"\n\nmart_city_top_products.write.mode(\"overwrite\").parquet(s3_path)\n\nprint(f\"Записано в {s3_path}\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# Проверим теперь данные, которые были записаны в HDFS\nresult \u003d spark.read.parquet(hdfs_path)\n\nresult.orderBy(\"city\", \"rank\").show()"
    }
  ]
}